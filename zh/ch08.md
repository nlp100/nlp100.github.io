---
title: "第8章：神经网络"
date: 2020-09-05 16:30:00 +0800
layout: single
toc: true
lang: zh
sidebar: {nav: "sidebar_zh"}
header:
  teaser: /assets/images/ch08.png
---


在本章中，我们需要实现一个神经网络以进行文本分类任务，随后将该模型应用至第6章中的新闻分类任务。你还可以考虑使用一些深度学习框架，例如PyTorch, TensorFlow, Chainer。

## 70. 基于词向量之和来生成特征

让我们考虑将50问中构建的训练数据集、验证数据集、测试数据集转换为特征向量。例如，对于训练数据，利用每个实例$$x_i$$的特征向量$$\boldsymbol{x}_i$$拼接创建矩阵$$X$$来代表所有输入，而向量$$Y$$代表所有实例对应的正确标注。

$$
X = \begin{pmatrix} 
  \boldsymbol{x}_1 \\ 
  \boldsymbol{x}_2 \\ 
  \dots \\ 
  \boldsymbol{x}_n \\ 
\end{pmatrix} \in \mathbb{R}^{n \times d},
Y = \begin{pmatrix} 
  y_1 \\ 
  y_2 \\ 
  \dots \\ 
  y_n \\ 
\end{pmatrix} \in \mathbb{N}^{n}
$$

在这里， $$n$$代表训练数据集中的实例数量。 $$\boldsymbol{x}_i \in \mathbb{R}^d$$ 以及 $$y_i \in \mathbb{N}$$ 分别代表第$$i \in {1,...,n}$$个特征向量以及它所对应的正确标注。
请注意这里的任务是将给定的新闻语料分类至"Business", "Science", "Entertainment" 和 "Health"四个分类的其中一类。
我们设 $$\mathbb{N}_4$$ 代表小于4的自然数（包括0），任何给定实例的标注均可以被表示为$$y_i \in \mathbb{N}_4$$。
同时设$$L$$代表标注的数值 (这里$$L=4$$)。

第$$i$$个实例的特征向量$$\boldsymbol{x}_i$$可以通过如下方式计算： 

$$
\boldsymbol{x}_i = \frac{1}{T_i} \sum_{t=1}^{T_i} \mathrm{emb}(w_{i,t}),
$$

其中第$$i$$个实例由$$T_i$$个token的表示序列$$(w_{i,1},...,w_{i,T_i})$$构成，其中词$$w$$对应的词向量为$$emb(w) \in \mathbb{R}^{d}$$（维度大小为 $$d$$）。
我们可以直接使用60问中所提供的预训练词向量，因此词向量的维度大小为300($$d=300$$)。

此外，第i个实例的标注$$y_i$$按如下方式定义：

$$
y_i = \begin{cases}
0 & (\mbox{if article }\boldsymbol{x}_i\mbox{ belongs to Business category}) \\
1 & (\mbox{if article }\boldsymbol{x}_i\mbox{ belongs to Science category}) \\
2 & (\mbox{if article }\boldsymbol{x}_i\mbox{ belongs to Entertainment category}) \\
3 & (\mbox{if article }\boldsymbol{x}_i\mbox{ belongs to Health category}) \\
\end{cases}
$$

请注意这里你并不需要严格按照上述方法进行定义，只要分类与标注索引之间存在一对一的映射关系即可。

基于上述规定, 创建以下矩阵和向量并将它们保存到二进制文件中：

+ 训练数据特征矩阵: $$X_{\rm train} \in \mathbb{R}^{N_t \times d}$$
+ 训练数据标注向量: $$Y_{\rm train} \in \mathbb{N}^{N_t}$$
+ 验证数据特征矩阵: $$X_{\rm valid} \in \mathbb{R}^{N_v \times d}$$
+ 验证数据标注向量: $$Y_{\rm valid} \in \mathbb{N}^{N_v}$$
+ 测试数据特征矩阵: $$X_{\rm test} \in \mathbb{R}^{N_e \times d}$$
+ 测试数据标注向量: $$Y_{\rm test} \in \mathbb{N}^{N_e}$$

在这里, $$N_t, N_v, N_e$$分别代表训练数据、验证数据、测试数据中的实例数量。

## 71. 搭建一个单层神经网络

从70问结果中载入相应矩阵与向量。按照下列步骤在训练数据集上进行计算：

$$
\hat{\boldsymbol{y}}_1 = {\rm softmax}(\boldsymbol{x}_1 W), \\
\hat{Y} = {\rm softmax}(X_{[1:4]} W)
$$

其中，$${\rm softmax}$$指softmax函数，$$X_{[1:4]} \in \mathbb{R}^{4 \times d}$$是由$$\boldsymbol{x}_1, \boldsymbol{x}_2, \boldsymbol{x}_3, \boldsymbol{x}_4$$纵向连接而成:

$$
X_{[1:4]} = \begin{pmatrix} 
  \boldsymbol{x}_1 \\ 
  \boldsymbol{x}_2 \\ 
  \boldsymbol{x}_3 \\ 
  \boldsymbol{x}_4 \\ 
\end{pmatrix}
$$

矩阵$$W \in \mathbb{R}^{d \times L}$$是单层神经网络的权重.
你可以暂时随机初始化这个权重（在后续问题中我们将更新这个权重的初始化方式）。
请注意$$\hat{\boldsymbol{y}}_1 \in \mathbb{R}^L$$代表在所有分类上的概率分布。
相似的，$$\hat{Y} \in \mathbb{R}^{n \times L}$$代表训练数据实例$$x_1, x_2, x_3, x_4$$属于各类别的概率。

## 72. 计算损失(loss)及梯度(gradients)

在训练样本$$x_1$$与训练样本集合$$x_1, x_2, x_3, x_4$$上分别计算交叉熵损失，以及对矩阵$$W$$的梯度。在单个样本上计算损失所用的公式如下：

$$
l_i = - \log [\mbox{样本}x_i\mbox{被分类为}y_i\mbox{的概率}]
$$

此外，一个训练集合的交叉熵损失是该集合中各样本损失的平均值。

## 73. 使用随机梯度下降法来学习

使用随机梯度下降法(SGD)来更新矩阵$$W$$。
训练过程中，应该选择合适的基准来进行终止，例如“在100个epochs后终止”。

## 74. 计算准确率

使用73问中得到的模型权重矩阵，在训练与验证集上计算分类准确率。

## 75. 绘制损失-准确率图

修改73问中的代码，绘制出在训练损失和在验证集上的正确率，绘制出以epoch为单位的变化图。
使用该图对学习的进程进行监控。

## 76. 检查点(Checkpoints)

修改75问中的代码，使得在每个epoch训练完成后保存检查点到磁盘。
其中，检查点应该包括模型参数的值，例如权重矩阵、优化算法的状态。

## 77. Mini-batches

修改问题76中的代码，使得以每$$B$$个样本为单位计算损失/梯度并更新权重矩阵$$W$$的值(mini-batch)。改变$$B$$的值为$$1, 2, 4, 8, \dots$$，比较不同情况下训练一个epoch所耗费的时间。

## 78. GPU训练

修改77问中的代码使得训练能够在GPU上进行。

## 79. 多层神经网络

修改78问中的代码，通过改变为多层网络架构、引入偏置项(bias)等操作，尝试创建一个高性能的分类器。

