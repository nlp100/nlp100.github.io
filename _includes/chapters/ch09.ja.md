## 第9章: RNN, CNN, アテンション

### 80. ID番号への変換

問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に`1`，2番目に頻出する単語に`2`，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて`0`とせよ．

### 81. RNNによる予測

ID番号で表現された単語列$$\boldsymbol{x} = (x_1, x_2, \dots, x_T)$$がある．ただし，$$T$$は単語列の長さ，$$x_t \in \mathbb{R}^{V}$$は単語のID番号のone-hot表記である（$$V$$は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列$$\boldsymbol{x}$$からカテゴリ$$y$$を予測するモデルとして，次式を実装せよ．

$$
\overrightarrow{h}_0 = 0, \\
\overrightarrow{h}_t = {\rm \overrightarrow{RNN}}(\mathrm{emb}(x_t), \overrightarrow{h}_{t-1}), \\
y = {\rm softmax}(W^{(yh)} \overrightarrow{h}_T + b^{(y)})
$$

ただし，$$\mathrm{emb}(x) \in \mathbb{R}^{d_w}$$は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），$$\overrightarrow{h}_t \in \mathbb{R}^{d_h}$$は時刻$$t$$の隠れ状態ベクトル，$${\rm \overrightarrow{RNN}}(x,h)$$は入力$$x$$と前時刻の隠れ状態$$h$$から次状態を計算するRNNユニット，$$W^{(yh)} \in \mathbb{R}^{L \times d_h}$$は隠れ状態ベクトルからカテゴリを予測するための行列，$$b^{(y)} \in \mathbb{R}^{L}$$はバイアス項である（$$d_w, d_h, L$$はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット$${\rm \overrightarrow{RNN}}(x,h)$$には様々な構成が考えられるが，典型例として次式が挙げられる．

$$
{\rm \overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)})
$$

ただし，$$W^{(hx)} \in \mathbb{R}^{d_h \times d_w}，W^{(hh)} \in \mathbb{R}^{d_h \times d_h}, b^{(h)} \in \mathbb{R}^{d_h}$$はRNNユニットのパラメータ，$$g$$は活性化関数（例えば$$\tanh$$やReLUなど）である．

なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータで$$y$$を計算するだけでよい．次元数などのハイパーパラメータは，$$d_w = 300, d_h=50$$など，適当な値に設定せよ（以降の問題でも同様である）．

### 82. 確率的勾配降下法による学習

確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．

### 83. ミニバッチ化・GPU上での学習

問題82のコードを改変し，$$B$$事例ごとに損失・勾配を計算して学習を行えるようにせよ（$$B$$の値は適当に選べ）．また，GPU上で学習を実行せよ．

### 84. 単語ベクトルの導入

事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込み$$\mathrm{emb}(x)$$を初期化し，学習せよ．

### 85. 双方向RNN・多層化

順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．

$$
\overleftarrow{h}_{T+1} = 0, \\
\overleftarrow{h}_t = {\rm \overleftarrow{RNN}}(\mathrm{emb}(x_t), \overleftarrow{h}_{t+1}), \\
y = {\rm softmax}(W^{(yh)} [\overrightarrow{h}_T; \overleftarrow{h}_1] + b^{(y)})
$$

ただし，$$\overrightarrow{h}_t \in \mathbb{R}^{d_h}, \overleftarrow{h}_t \in \mathbb{R}^{d_h}$$はそれぞれ，順方向および逆方向のRNNで求めた時刻$$t$$の隠れ状態ベクトル，$${\rm \overleftarrow{RNN}}(x,h)$$は入力$$x$$と次時刻の隠れ状態$$h$$から前状態を計算するRNNユニット，$$W^{(yh)} \in \mathbb{R}^{L \times 2d_h}$$は隠れ状態ベクトルからカテゴリを予測するための行列，$$b^{(y)} \in \mathbb{R}^{L}$$はバイアス項である．また，$$[a; b]$$はベクトル$$a$$と$$b$$の連結を表す。

さらに，双方向RNNを多層化して実験せよ．

### 86. 畳み込みニューラルネットワーク (CNN)

ID番号で表現された単語列$$\boldsymbol{x} = (x_1, x_2, \dots, x_T)$$がある．ただし，$$T$$は単語列の長さ，$$x_t \in \mathbb{R}^{V}$$は単語のID番号のone-hot表記である（$$V$$は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列$$\boldsymbol{x}$$からカテゴリ$$y$$を予測するモデルを実装せよ．

ただし，畳み込みニューラルネットワークの構成は以下の通りとする．

+ 単語埋め込みの次元数: $$d_w$$
+ 畳み込みのフィルターのサイズ: 3 トークン
+ 畳み込みのストライド: 1 トークン
+ 畳み込みのパディング: あり
+ 畳み込み演算後の各時刻のベクトルの次元数: $$d_h$$
+ 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を$$d_h$$次元の隠れベクトルで表現

すなわち，時刻$$t$$の特徴ベクトル$$p_t \in \mathbb{R}^{d_h}$$は次式で表される．

$$
p_t = g(W^{(px)} [\mathrm{emb}(x_{t-1}); \mathrm{emb}(x_t); \mathrm{emb}(x_{t+1})] + b^{(p)})
$$

ただし，$$W^{(px)} \in \mathbb{R}^{d_h \times 3d_w}, b^{(p)} \in \mathbb{R}^{d_h}$$はCNNのパラメータ，$$g$$は活性化関数（例えば$$\tanh$$やReLUなど），$$[a; b; c]$$はベクトル$$a, b, c$$の連結である．なお，行列$$W^{(px)}$$の列数が$$3d_w$$になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．

最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル$$c \in \mathbb{R}^{d_h}$$を求める．$$c[i]$$でベクトル$$c$$の$$i$$番目の次元の値を表すことにすると，最大値プーリングは次式で表される．

$$
c[i] = \max_{1 \leq t \leq T} p_t[i]
$$

最後に，入力文書の特徴ベクトル$$c$$に行列$$W^{(yc)} \in \mathbb{R}^{L \times d_h}$$とバイアス項$$b^{(y)} \in \mathbb{R}^{L}$$による線形変換とソフトマックス関数を適用し，カテゴリ$$y$$を予測する．

$$
y = {\rm softmax}(W^{(yc)} c + b^{(y)})
$$

なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$$y$$を計算するだけでよい．

### 87. 確率的勾配降下法によるCNNの学習

確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．

### 88. パラメータチューニング

問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．

### 89. 事前学習済み言語モデルからの転移学習

事前学習済み言語モデル（例えば[BERT](https://github.com/google-research/bert)など）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．
